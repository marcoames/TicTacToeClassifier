{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  2  3  4  5  6  7  8  9 result\n",
      "0  0  0  1  1  2  1  2  2  2      1\n",
      "1  0  0  1  2  1  1  2  2  0      3\n",
      "2  0  2  2  2  1  1  1  2  1      3\n",
      "3  2  0  2  1  0  1  0  2  1      3\n",
      "4  2  1  0  2  0  2  0  0  1      3\n",
      "5  0  0  1  0  1  2  0  2  0      3\n",
      "6  0  2  2  0  1  1  1  2  0      3\n",
      "7  2  0  1  0  0  1  0  2  0      3\n",
      "8  0  0  2  1  0  1  2  0  0      3\n",
      "9  0  2  0  1  1  0  2  0  0      3\n",
      "Tamanho do conjunto de treino: (3898, 9) - 80%\n",
      "Tamanho do conjunto de validação: (487, 9) - 10%\n",
      "Tamanho do conjunto de teste: (488, 9) - 10%\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('data/dataset.csv')\n",
    "\n",
    "# encode x-> 2, o-> 1, b-> 0\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "encoded_dataset = dataset.replace({'x': 2, 'o': 1, 'b': 0})\n",
    "\n",
    "# encode Xganha->1, 0ganha->2, Temjogo->3, Empate->4\n",
    "encoded_dataset = encoded_dataset.replace({'Xganha': 1, 'Oganha': 2, 'Temjogo': 3, 'Empate': 4}) \n",
    "\n",
    "print(encoded_dataset.head(10))\n",
    "\n",
    "# Separa em treino e teste\n",
    "\n",
    "# Separa as features (X) e o rótulo (y)\n",
    "X = encoded_dataset.drop('result', axis=1)\n",
    "y = encoded_dataset['result'] \n",
    "\n",
    "X = X.astype(int)\n",
    "y = y.astype(int)\n",
    "\n",
    "# 80% treino e 20% temporário\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Divide o conjunto temporário em 50% validação e 50% teste\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Total 80% treino 10% validação e 10% teste\n",
    "\n",
    "# Exibe o tamanho dos conjuntos\n",
    "print(f\"Tamanho do conjunto de treino: {X_train.shape} - 80%\")\n",
    "print(f\"Tamanho do conjunto de validação: {X_val.shape} - 10%\")\n",
    "print(f\"Tamanho do conjunto de teste: {X_test.shape} - 10%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuição dos rótulos (y_train):\n",
      "\n",
      "1: Xganha, 2: Oganha, 3: Temjogo, 4: Empate\n",
      "result\n",
      "3    2908\n",
      "1     703\n",
      "2     276\n",
      "4      11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Exibir a contagem de valores para o conjunto de treino e os rótulos\n",
    "\n",
    "print(\"\\nDistribuição dos rótulos (y_train):\")\n",
    "print(\"\\n1: Xganha, 2: Oganha, 3: Temjogo, 4: Empate\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuição dos rótulos (y_val):\n",
      "\n",
      "1: Xganha, 2: Oganha, 3: Temjogo, 4: Empate\n",
      "result\n",
      "3    360\n",
      "1     94\n",
      "2     32\n",
      "4      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Exibir a contagem de valores para o conjunto de validacao e os rótulos\n",
    "\n",
    "print(\"\\nDistribuição dos rótulos (y_val):\")\n",
    "print(\"\\n1: Xganha, 2: Oganha, 3: Temjogo, 4: Empate\")\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuição dos rótulos (y_test):\n",
      "\n",
      "1: Xganha, 2: Oganha, 3: Temjogo, 4: Empate\n",
      "result\n",
      "3    383\n",
      "1     83\n",
      "2     22\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Exibir a contagem de valores para o conjunto de teste e os rótulos\n",
    "print(\"\\nDistribuição dos rótulos (y_test):\")\n",
    "print(\"\\n1: Xganha, 2: Oganha, 3: Temjogo, 4: Empate\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, Accuracy=0.8704\n",
      "k=2, Accuracy=0.8330\n",
      "k=3, Accuracy=0.8923\n",
      "k=4, Accuracy=0.8920\n",
      "k=5, Accuracy=0.8974\n",
      "k=6, Accuracy=0.8992\n",
      "k=7, Accuracy=0.8969\n",
      "k=8, Accuracy=0.9012\n",
      "k=9, Accuracy=0.8915\n",
      "k=10, Accuracy=0.8966\n",
      "\n",
      "Best k value: 8\n",
      "Best accuracy: 0.9012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# calcula a acurácia para cada valor de k\n",
    "k_values = range(1, 11) \n",
    "accuracy_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "    mean_score = scores.mean()\n",
    "    accuracy_scores.append(mean_score)\n",
    "    print(f\"k={k}, Accuracy={mean_score:.4f}\")\n",
    "\n",
    "# acha melhor k\n",
    "best_k = k_values[np.argmax(accuracy_scores)]\n",
    "best_accuracy = max(accuracy_scores)\n",
    "\n",
    "print(f\"\\nBest k value: {best_k}\")\n",
    "print(f\"Best accuracy: {best_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.89      0.91        94\n",
      "           2       0.78      0.22      0.34        32\n",
      "           3       0.92      0.99      0.95       360\n",
      "           4       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.92       487\n",
      "   macro avg       0.66      0.53      0.55       487\n",
      "weighted avg       0.91      0.92      0.90       487\n",
      "\n",
      "Validation Accuracy: 0.9179\n",
      "Test Accuracy: 0.9303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/knn_model.joblib']"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Treina o modelo\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de validação\n",
    "y_pred = knn.predict(X_val)\n",
    "\n",
    "# Exibir o relatório de classificação\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(report)\n",
    "\n",
    "val_predictions = knn.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "test_predictions = knn.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Salvando o modelo e a acurácia juntos\n",
    "model_data = {\n",
    "    'model': knn,\n",
    "    'accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "joblib.dump(model_data, 'models/knn_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.95      0.95        94\n",
      "           2       0.67      0.50      0.57        32\n",
      "           3       0.95      0.97      0.96       360\n",
      "           4       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.93       487\n",
      "   macro avg       0.89      0.85      0.87       487\n",
      "weighted avg       0.93      0.93      0.93       487\n",
      "\n",
      "Validation Accuracy: 0.9343\n",
      "Test Accuracy: 0.9426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/mlp_model.joblib']"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Treina o modelo\n",
    "mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(100,), learning_rate_init=0.001, momentum=0.01)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de validação\n",
    "y_pred = mlp.predict(X_val)\n",
    "\n",
    "# Exibir o relatório de classificação\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(report)\n",
    "\n",
    "val_predictions = mlp.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "test_predictions = mlp.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Salvando o modelo e a acurácia juntos\n",
    "model_data = {\n",
    "    'model': mlp,\n",
    "    'accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "# Salvando o modelo\n",
    "joblib.dump(model_data, 'models/mlp_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.71      0.71        94\n",
      "           2       0.17      0.16      0.16        32\n",
      "           3       0.88      0.89      0.89       360\n",
      "           4       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80       487\n",
      "   macro avg       0.44      0.44      0.44       487\n",
      "weighted avg       0.80      0.80      0.80       487\n",
      "\n",
      "Validation Accuracy: 0.8049\n",
      "Test Accuracy: 0.8381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/decision_tree_model.joblib']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Treina o modelo\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de validação\n",
    "y_pred = dtree.predict(X_val)\n",
    "\n",
    "# Exibir o relatório de classificação\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(report)\n",
    "\n",
    "val_predictions = dtree.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "test_predictions = dtree.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Salvando o modelo e a acurácia juntos\n",
    "model_data = {\n",
    "    'model': dtree,\n",
    "    'accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "# Salvando o modelo\n",
    "joblib.dump(model_data, 'models/decision_tree_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.86      0.89        94\n",
      "           2       0.86      0.19      0.31        32\n",
      "           3       0.92      1.00      0.96       360\n",
      "           4       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.92       487\n",
      "   macro avg       0.67      0.51      0.54       487\n",
      "weighted avg       0.91      0.92      0.90       487\n",
      "\n",
      "Validation Accuracy: 0.9179\n",
      "Test Accuracy: 0.9488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/random_forest_model.joblib']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)  # 100 árvores\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de validação\n",
    "y_pred = rf.predict(X_val)\n",
    "\n",
    "# Exibir o relatório de classificação\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(report)\n",
    "\n",
    "val_predictions = rf.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "test_predictions = rf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Salvando o modelo e a acurácia juntos\n",
    "model_data = {\n",
    "    'model': rf,\n",
    "    'accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "# Salvando o modelo\n",
    "joblib.dump(model_data, 'models/random_forest_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
